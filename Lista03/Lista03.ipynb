{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "import matplotlib.cm as cm\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras import losses\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset MNIST tem 60000 exemplos de treino\n",
      "O dataset MNIST tem 10000 exemplos de teste\n"
     ]
    }
   ],
   "source": [
    "print(\"O dataset MNIST tem %d exemplos de treino\" % len(X_train))\n",
    "print(\"O dataset MNIST tem %d exemplos de teste\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotando algumas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADVCAYAAADQKmyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAkwxAEAAABIwP8BwWNz9czkxywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1440 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " fig = plt.figure(figsize=(20,20))\n",
    "for i in range (5):\n",
    "    im = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n",
    "    im.imshow(X_train[i], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escalando as imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escala [0,255] --> [0,1]\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255 \n",
    "\n",
    "X_train[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model(p):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units= 64, activation='relu'))\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 121,930\n",
      "Trainable params: 121,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels) #Converts a class vector (integers) to binary class matrix.\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = train_images[:6000, :, :]\n",
    "X_train_labels = train_labels[:6000, :]\n",
    "\n",
    "X_valid = train_images[6000:7000, :, :]\n",
    "X_valid_labels= train_labels[6000:7000, :]\n",
    "\n",
    "X_test = test_images[:1000, :, :]\n",
    "X_test_labels = test_labels[:1000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = rmsprop  e loss =  categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 6s 1ms/step - loss: 0.6634 - acc: 0.7952 - val_loss: 0.2409 - val_acc: 0.9280\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 5s 798us/step - loss: 0.1796 - acc: 0.9482 - val_loss: 0.1373 - val_acc: 0.9630\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 735us/step - loss: 0.1235 - acc: 0.9645 - val_loss: 0.1612 - val_acc: 0.9500\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 750us/step - loss: 0.0860 - acc: 0.9768 - val_loss: 0.1292 - val_acc: 0.9650\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 739us/step - loss: 0.0664 - acc: 0.9798 - val_loss: 0.0956 - val_acc: 0.9740\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 0.0498 - acc: 0.9873 - val_loss: 0.1168 - val_acc: 0.9620\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 818us/step - loss: 0.0344 - acc: 0.9912 - val_loss: 0.1277 - val_acc: 0.9640\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 6s 920us/step - loss: 0.0313 - acc: 0.9918 - val_loss: 0.1449 - val_acc: 0.9660\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 6s 957us/step - loss: 0.0222 - acc: 0.9933 - val_loss: 0.1083 - val_acc: 0.9760\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 6s 933us/step - loss: 0.0197 - acc: 0.9948 - val_loss: 0.1073 - val_acc: 0.9710\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = SGD e loss =  categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 4s 744us/step - loss: 0.0030 - acc: 0.9997 - val_loss: 0.0981 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 744us/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0981 - val_acc: 0.9790\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 5s 759us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0991 - val_acc: 0.9790\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 5s 766us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.0987 - val_acc: 0.9790\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.0987 - val_acc: 0.9810\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 709us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.0991 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 911us/step - loss: 0.0025 - acc: 0.9998 - val_loss: 0.0997 - val_acc: 0.9790\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 6s 1ms/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.1001 - val_acc: 0.9800\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 910us/step - loss: 0.0024 - acc: 0.9998 - val_loss: 0.1006 - val_acc: 0.9800\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 6s 948us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1011 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "history1 = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = SGDe loss =  categorical_hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 804us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9800\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 5s 850us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9800\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 5s 867us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.0468 - val_acc: 0.9800\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 6s 938us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.0470 - val_acc: 0.9800\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 793us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9800\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 862us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9800\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 844us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9800\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 830us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9800\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 857us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9800\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 827us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "loss=losses.categorical_hinge,\n",
    "metrics=['accuracy'])\n",
    "history2 = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = SGD  e loss =  mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 768us/step - loss: 2.5808e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 699us/step - loss: 2.5757e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 702us/step - loss: 2.5685e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 705us/step - loss: 2.5637e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 778us/step - loss: 2.5571e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 694us/step - loss: 2.5523e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 866us/step - loss: 2.5451e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 6s 933us/step - loss: 2.5393e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 861us/step - loss: 2.5331e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 859us/step - loss: 2.5263e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "loss=losses.mean_absolute_error,\n",
    "metrics=['accuracy'])\n",
    "history3 = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = ADELTA  e loss =  mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 820us/step - loss: 1.8332e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 5s 755us/step - loss: 1.5638e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9800\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 717us/step - loss: 1.3101e-04 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9790\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 728us/step - loss: 1.2102e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9790\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 864us/step - loss: 1.0086e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 6s 965us/step - loss: 9.7161e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9780\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 6s 963us/step - loss: 8.6469e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9780\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 6s 942us/step - loss: 7.8050e-05 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9780\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 6s 972us/step - loss: 7.2036e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9790\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 6s 962us/step - loss: 6.7736e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9790\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=adelta,\n",
    "loss=losses.mean_absolute_error,\n",
    "metrics=['accuracy'])\n",
    "history4 = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = SGD  e loss =  cosine_proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 806us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 711us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 714us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 719us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 760us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 754us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 904us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 6s 935us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 6s 930us/step - loss: -1.0000 - acc: 1.0000 - val_loss: -0.9815 - val_acc: 0.9790\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=sgd,\n",
    "loss=keras.losses.cosine_proximity,\n",
    "metrics=['accuracy'])\n",
    "history4 = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compiler(model):\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd,\n",
    "    loss=losses.categorical_hinge,\n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 1.0011 - acc: 0.1123 - val_loss: 1.0005 - val_acc: 0.1070\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 1.0006 - acc: 0.1098 - val_loss: 1.0003 - val_acc: 0.0910\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 1.0003 - acc: 0.0963 - val_loss: 1.0002 - val_acc: 0.0870\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 1.0002 - acc: 0.1017 - val_loss: 1.0001 - val_acc: 0.0850\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 6s 21ms/step - loss: 1.0001 - acc: 0.1093 - val_loss: 1.0001 - val_acc: 0.0900\n"
     ]
    }
   ],
   "source": [
    "historyD0 = model.fit_generator(datagen.flow(X_train, X_train_labels, batch_size=20), epochs=5, \n",
    "                                validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optmizer = SGDe loss =  categorical_hinge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "300/300 [==============================] - 7s 24ms/step - loss: 2.0318 - acc: 0.2668 - val_loss: 1.2355 - val_acc: 0.6610\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 1.6718 - acc: 0.4165 - val_loss: 0.9162 - val_acc: 0.7580\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 6s 21ms/step - loss: 1.4635 - acc: 0.4913 - val_loss: 0.8163 - val_acc: 0.7440\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 1.2952 - acc: 0.5637 - val_loss: 0.6455 - val_acc: 0.8090\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 7s 24ms/step - loss: 1.1868 - acc: 0.6073 - val_loss: 0.5725 - val_acc: 0.8200\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 8s 26ms/step - loss: 1.1010 - acc: 0.6277 - val_loss: 0.5933 - val_acc: 0.7870\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 1.0187 - acc: 0.6528 - val_loss: 0.5336 - val_acc: 0.8390\n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 0.9619 - acc: 0.6830 - val_loss: 0.6396 - val_acc: 0.7920\n",
      "Epoch 9/100\n",
      "300/300 [==============================] - 7s 23ms/step - loss: 0.9002 - acc: 0.7040 - val_loss: 0.4419 - val_acc: 0.8580\n",
      "Epoch 10/100\n",
      "300/300 [==============================] - 7s 24ms/step - loss: 0.8642 - acc: 0.7165 - val_loss: 0.3469 - val_acc: 0.8950\n",
      "Epoch 11/100\n",
      "300/300 [==============================] - 8s 25ms/step - loss: 0.8343 - acc: 0.7202 - val_loss: 0.3563 - val_acc: 0.8790\n",
      "Epoch 12/100\n",
      "300/300 [==============================] - 8s 25ms/step - loss: 0.8002 - acc: 0.7355 - val_loss: 0.2753 - val_acc: 0.9120\n",
      "Epoch 13/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.7685 - acc: 0.7493 - val_loss: 0.3538 - val_acc: 0.8900\n",
      "Epoch 14/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.7675 - acc: 0.7517 - val_loss: 0.3533 - val_acc: 0.8940\n",
      "Epoch 15/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.7133 - acc: 0.7692 - val_loss: 0.2947 - val_acc: 0.9100\n",
      "Epoch 16/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6974 - acc: 0.7820 - val_loss: 0.2707 - val_acc: 0.9160\n",
      "Epoch 17/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.7027 - acc: 0.7715 - val_loss: 0.2454 - val_acc: 0.9300\n",
      "Epoch 18/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6746 - acc: 0.7870 - val_loss: 0.2447 - val_acc: 0.9190\n",
      "Epoch 19/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6479 - acc: 0.7873 - val_loss: 0.2865 - val_acc: 0.9220\n",
      "Epoch 20/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6529 - acc: 0.7865 - val_loss: 0.3133 - val_acc: 0.9150\n",
      "Epoch 21/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6156 - acc: 0.8017 - val_loss: 0.2552 - val_acc: 0.9280\n",
      "Epoch 22/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6441 - acc: 0.7918 - val_loss: 0.2235 - val_acc: 0.9410\n",
      "Epoch 23/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6050 - acc: 0.8072 - val_loss: 0.2392 - val_acc: 0.9270\n",
      "Epoch 24/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.6010 - acc: 0.8027 - val_loss: 0.2332 - val_acc: 0.9310\n",
      "Epoch 25/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5773 - acc: 0.8143 - val_loss: 0.2135 - val_acc: 0.9430\n",
      "Epoch 26/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5951 - acc: 0.8052 - val_loss: 0.3808 - val_acc: 0.8870\n",
      "Epoch 27/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5782 - acc: 0.8183 - val_loss: 0.2108 - val_acc: 0.9410\n",
      "Epoch 28/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5794 - acc: 0.8167 - val_loss: 0.2558 - val_acc: 0.9250\n",
      "Epoch 29/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5743 - acc: 0.8195 - val_loss: 0.2133 - val_acc: 0.9390\n",
      "Epoch 30/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5528 - acc: 0.8217 - val_loss: 0.2100 - val_acc: 0.9390\n",
      "Epoch 31/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5606 - acc: 0.8260 - val_loss: 0.1961 - val_acc: 0.9350\n",
      "Epoch 32/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5532 - acc: 0.8227 - val_loss: 0.1962 - val_acc: 0.9380\n",
      "Epoch 33/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5480 - acc: 0.8225 - val_loss: 0.1673 - val_acc: 0.9470\n",
      "Epoch 34/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5489 - acc: 0.8285 - val_loss: 0.2460 - val_acc: 0.9280\n",
      "Epoch 35/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5336 - acc: 0.8288 - val_loss: 0.1741 - val_acc: 0.9430\n",
      "Epoch 36/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5363 - acc: 0.8310 - val_loss: 0.1666 - val_acc: 0.9540\n",
      "Epoch 37/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5308 - acc: 0.8372 - val_loss: 0.1750 - val_acc: 0.9500\n",
      "Epoch 38/100\n",
      "300/300 [==============================] - 6s 21ms/step - loss: 0.5291 - acc: 0.8318 - val_loss: 0.1605 - val_acc: 0.9530\n",
      "Epoch 39/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5053 - acc: 0.8462 - val_loss: 0.1545 - val_acc: 0.9550\n",
      "Epoch 40/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5193 - acc: 0.8400 - val_loss: 0.1626 - val_acc: 0.9540\n",
      "Epoch 41/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5144 - acc: 0.8357 - val_loss: 0.2771 - val_acc: 0.9210\n",
      "Epoch 42/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5203 - acc: 0.8393 - val_loss: 0.1612 - val_acc: 0.9540\n",
      "Epoch 43/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5058 - acc: 0.8413 - val_loss: 0.2543 - val_acc: 0.9290\n",
      "Epoch 44/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5084 - acc: 0.8428 - val_loss: 0.2025 - val_acc: 0.9440\n",
      "Epoch 45/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4954 - acc: 0.8447 - val_loss: 0.1721 - val_acc: 0.9480\n",
      "Epoch 46/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5040 - acc: 0.8400 - val_loss: 0.1638 - val_acc: 0.9530\n",
      "Epoch 47/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5074 - acc: 0.8400 - val_loss: 0.1860 - val_acc: 0.9450\n",
      "Epoch 48/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4858 - acc: 0.8507 - val_loss: 0.1702 - val_acc: 0.9520\n",
      "Epoch 49/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.5143 - acc: 0.8452 - val_loss: 0.1859 - val_acc: 0.9450\n",
      "Epoch 50/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4920 - acc: 0.8438 - val_loss: 0.1925 - val_acc: 0.9390\n",
      "Epoch 51/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4791 - acc: 0.8507 - val_loss: 0.2113 - val_acc: 0.9430\n",
      "Epoch 52/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4848 - acc: 0.8520 - val_loss: 0.2085 - val_acc: 0.9380\n",
      "Epoch 53/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4689 - acc: 0.8513 - val_loss: 0.1839 - val_acc: 0.9440\n",
      "Epoch 54/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4893 - acc: 0.8460 - val_loss: 0.1516 - val_acc: 0.9520\n",
      "Epoch 55/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4736 - acc: 0.8552 - val_loss: 0.1848 - val_acc: 0.9470\n",
      "Epoch 56/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4872 - acc: 0.8463 - val_loss: 0.1893 - val_acc: 0.9390\n",
      "Epoch 57/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4804 - acc: 0.8520 - val_loss: 0.2046 - val_acc: 0.9440\n",
      "Epoch 58/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4849 - acc: 0.8492 - val_loss: 0.1538 - val_acc: 0.9550\n",
      "Epoch 59/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4966 - acc: 0.8458 - val_loss: 0.2500 - val_acc: 0.9350\n",
      "Epoch 60/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4696 - acc: 0.8560 - val_loss: 0.1707 - val_acc: 0.9540\n",
      "Epoch 61/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4623 - acc: 0.8607 - val_loss: 0.2707 - val_acc: 0.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4673 - acc: 0.8577 - val_loss: 0.2448 - val_acc: 0.9350\n",
      "Epoch 63/100\n",
      "300/300 [==============================] - 7s 22ms/step - loss: 0.4672 - acc: 0.8520 - val_loss: 0.1912 - val_acc: 0.9490\n",
      "Epoch 64/100\n",
      "300/300 [==============================] - 6s 21ms/step - loss: 0.4863 - acc: 0.8497 - val_loss: 0.1607 - val_acc: 0.9520\n",
      "Epoch 65/100\n",
      "300/300 [==============================] - 7s 22ms/step - loss: 0.4970 - acc: 0.8482 - val_loss: 0.1965 - val_acc: 0.9460\n",
      "Epoch 66/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4827 - acc: 0.8478 - val_loss: 0.1557 - val_acc: 0.9510\n",
      "Epoch 67/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4718 - acc: 0.8540 - val_loss: 0.2993 - val_acc: 0.9150\n",
      "Epoch 68/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4653 - acc: 0.8550 - val_loss: 0.2233 - val_acc: 0.9370\n",
      "Epoch 69/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4803 - acc: 0.8503 - val_loss: 0.1697 - val_acc: 0.9510\n",
      "Epoch 70/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4791 - acc: 0.8557 - val_loss: 0.2023 - val_acc: 0.9440\n",
      "Epoch 71/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4767 - acc: 0.8553 - val_loss: 0.1845 - val_acc: 0.9490\n",
      "Epoch 72/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4746 - acc: 0.8560 - val_loss: 0.1788 - val_acc: 0.9480\n",
      "Epoch 73/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4671 - acc: 0.8595 - val_loss: 0.1602 - val_acc: 0.9500\n",
      "Epoch 74/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4542 - acc: 0.8597 - val_loss: 0.2143 - val_acc: 0.9450\n",
      "Epoch 75/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4520 - acc: 0.8610 - val_loss: 0.2431 - val_acc: 0.9280\n",
      "Epoch 76/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4451 - acc: 0.8560 - val_loss: 0.2260 - val_acc: 0.9370\n",
      "Epoch 77/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4646 - acc: 0.8572 - val_loss: 0.2073 - val_acc: 0.9390\n",
      "Epoch 78/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4638 - acc: 0.8548 - val_loss: 0.2026 - val_acc: 0.9430\n",
      "Epoch 79/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4549 - acc: 0.8567 - val_loss: 0.2250 - val_acc: 0.9350\n",
      "Epoch 80/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4755 - acc: 0.8543 - val_loss: 0.1789 - val_acc: 0.9480\n",
      "Epoch 81/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4541 - acc: 0.8583 - val_loss: 0.2301 - val_acc: 0.9310\n",
      "Epoch 82/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4473 - acc: 0.8595 - val_loss: 0.1542 - val_acc: 0.9530\n",
      "Epoch 83/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4600 - acc: 0.8572 - val_loss: 0.1883 - val_acc: 0.9430\n",
      "Epoch 84/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4480 - acc: 0.8587 - val_loss: 0.1864 - val_acc: 0.9480\n",
      "Epoch 85/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4440 - acc: 0.8655 - val_loss: 0.1642 - val_acc: 0.9580\n",
      "Epoch 86/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4523 - acc: 0.8582 - val_loss: 0.1813 - val_acc: 0.9470\n",
      "Epoch 87/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4477 - acc: 0.8592 - val_loss: 0.2299 - val_acc: 0.9320\n",
      "Epoch 88/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4285 - acc: 0.8698 - val_loss: 0.1470 - val_acc: 0.9530\n",
      "Epoch 89/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4539 - acc: 0.8568 - val_loss: 0.1584 - val_acc: 0.9540\n",
      "Epoch 90/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4458 - acc: 0.8623 - val_loss: 0.2077 - val_acc: 0.9440\n",
      "Epoch 91/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4620 - acc: 0.8582 - val_loss: 0.1960 - val_acc: 0.9450\n",
      "Epoch 92/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4387 - acc: 0.8652 - val_loss: 0.1875 - val_acc: 0.9490\n",
      "Epoch 93/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4441 - acc: 0.8700 - val_loss: 0.2224 - val_acc: 0.9420\n",
      "Epoch 94/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4357 - acc: 0.8665 - val_loss: 0.2250 - val_acc: 0.9450\n",
      "Epoch 95/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4487 - acc: 0.8595 - val_loss: 0.2262 - val_acc: 0.9380\n",
      "Epoch 96/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4615 - acc: 0.8590 - val_loss: 0.2000 - val_acc: 0.9410\n",
      "Epoch 97/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4482 - acc: 0.8598 - val_loss: 0.1842 - val_acc: 0.9470\n",
      "Epoch 98/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4521 - acc: 0.8590 - val_loss: 0.1802 - val_acc: 0.9500\n",
      "Epoch 99/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4332 - acc: 0.8693 - val_loss: 0.1717 - val_acc: 0.9410\n",
      "Epoch 100/100\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.4556 - acc: 0.8605 - val_loss: 0.2066 - val_acc: 0.9380\n"
     ]
    }
   ],
   "source": [
    "model = create_model(0.5)\n",
    "model = compiler(model)\n",
    "historyD0 = model.fit_generator(datagen.flow(X_train, X_train_labels, batch_size=20), epochs=100,\n",
    "                                validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
